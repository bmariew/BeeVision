{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This notebook details the complete workflow for building and evaluating a multi-label image classification model using the DenseNet201 architecture. The goal is to predict multiple labels simultaneously for each image within a dataset.  This notebook uses PyTorch for model building and training, scikit-learn for data splitting and some evaluation metrics.  The process is divided into several key steps, outlined below."
      ],
      "metadata": {
        "id": "bH63O5w3RYEH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This section imports all the necessary Python libraries required for the project.  Key libraries include PyTorch (for deep learning, scikit-learn (for machine learning utilities), pandas (for data manipulation), and several others for image processing, visualization, and progress tracking."
      ],
      "metadata": {
        "id": "7Wfmen3-Bo9L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpVDIsN5_hvG"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm.auto import tqdm\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from torchvision import models, transforms\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import itertools\n",
        "from sklearn.metrics import classification_report\n",
        "import random\n",
        "import json\n",
        "from plots import *\n",
        "from helper import *\n",
        "from metrics import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "ELhi6HF1vzd-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Files\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This step loads the image data and corresponding labels. The image paths are read from a CSV file ('annotations.csv'), which contains filenames and their associated labels with associated feature presence.  The images the dataset used for this project is a collection of images of beehive frames with varying features. The images themselves are loaded from the 'images/raw/' directory."
      ],
      "metadata": {
        "id": "r52ASpB1BubP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = 'data'\n",
        "csv_path = os.path.join(data_dir, 'annotations.csv')\n",
        "img_path = os.path.join(data_dir, 'images')\n",
        "df = pd.read_csv(csv_path)"
      ],
      "metadata": {
        "id": "8y2XdDtN_qli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definition and Training\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "A custom dataset class ('**CustomImageDataset**') is defined to efficiently handle the image and label data during training. The pre-trained DenseNet201 model is loaded, and its final classification layer is adjusted to match the number of output labels (15 in this case). The model is then trained using binary cross-entropy loss ('**BCEWithLogitsLoss**') and an Adam optimizer. The training process is monitored and logged, with metrics recorded at each epoch."
      ],
      "metadata": {
        "id": "y2qus4aaB4hU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, images, labels, data_transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = data_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Apply transformers if provided\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Convert label from a list to a tensor\n",
        "        label = torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "        sample = {'Image': image, 'Label': label}\n",
        "\n",
        "        return sample"
      ],
      "metadata": {
        "id": "OtuhZPfC_3Uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in label file (annotations.csv)\n",
        "df = pd.read_csv(csv_path)\n",
        "#df = df[:5]\n",
        "\n",
        "\n",
        "# Convert all labels to floats, NaN if blank\n",
        "for col in df.columns[1:16]:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "# Drop any images with NaNs in label columns\n",
        "df.dropna(subset=df.columns[:16], inplace=True)\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Extract ground truth labels for each image in label file (annotations.csv)\n",
        "labels = []\n",
        "for index, row in df.iterrows():\n",
        "    labels.append(list(row.iloc[1:16]))\n",
        "\n",
        "# Create path for and load in each image in directory\n",
        "image_names = df['filename'].tolist()\n",
        "images = []\n",
        "\n",
        "for img_name in tqdm(image_names):\n",
        "    image_path = os.path.join(img_path, img_name)\n",
        "    image = Image.open(image_path)\n",
        "    if image.mode != 'RGB':\n",
        "        image = image.convert('RGB')\n",
        "    images.append(image)\n",
        "\n",
        "# Split data into train (80%), validation (10%), and test (10%) sets\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(images, labels, test_size=1/10, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=1/9, random_state=42)"
      ],
      "metadata": {
        "id": "OQGOXgC0AGcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define image transformations\n",
        "\n",
        "densenet201_train_datatransform = transforms.Compose([\n",
        "    transforms.Resize(size=(224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "densenet201_test_datatransform = transforms.Compose([\n",
        "    transforms.Resize(size=(224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "_oHzZl38AIDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "train_data = CustomImageDataset(X_train, y_train, data_transform=densenet201_train_datatransform)\n",
        "val_data = CustomImageDataset(X_val, y_val, data_transform=densenet201_test_datatransform)\n",
        "test_data = CustomImageDataset(X_test, y_test, data_transform=densenet201_test_datatransform)\n",
        "\n",
        "# Create data loaders\n",
        "BATCH_SIZE = 32\n",
        "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Inherit the pre-trained weights from the DenseNet201 model\n",
        "weights = models.DenseNet201_Weights.DEFAULT\n",
        "model_d201 = models.densenet201(weights=weights).to(device)\n",
        "\n",
        "# Freeze all layers (optional)\n",
        "for param in model_d201.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Set output_shape variable to the number of labels in dataset\n",
        "input_shape = 1920\n",
        "output_shape = 15\n",
        "\n",
        "# Recreate the `classifier` layer of the pre-trained model to custom output shape\n",
        "model_d201.classifier = torch.nn.Sequential(\n",
        "    torch.nn.Linear(input_shape,\n",
        "                    output_shape),\n",
        ").to(device)\n",
        "\n",
        "# Training and Evaluation\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model_d201.parameters(), lr=0.001)\n",
        "\n",
        "densenet201_results = train(model=model_d201,\n",
        "                      train_dataloader=train_dataloader,\n",
        "                      test_dataloader=val_dataloader,\n",
        "                      optimizer=optimizer,\n",
        "                      loss_fn=criterion,\n",
        "                      epochs=10,\n",
        "                      device=device)"
      ],
      "metadata": {
        "id": "v6tYG-DNA8bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Testing on the Test Set ---\n",
        "test_loss, test_ham_acc, test_zero_one_acc = test_step(model=model_d201,\n",
        "                                                        dataloader=test_dataloader,\n",
        "                                                        loss_fn=criterion,\n",
        "                                                        device=device)\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Hamming Accuracy: {test_ham_acc:.4f}, Test Zero-One Accuracy: {test_zero_one_acc:.4f}\")"
      ],
      "metadata": {
        "id": "3Y-4-KEgwXlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparmeter Tuning\n",
        "---\n",
        "\n",
        "\n",
        "To optimize performance, a hyperparameter search is conducted. Differnt combinations of batch size, number of epochs, and learning rate are tested, using both the original data transformations and a new transformation using 'transforms.TriviaAugmentWide'. The results of this search are saved to 'hyperparameter_testing_densenet201.csv' for analysis. The best performing hyperparameter set is then saved to 'best_params_d201'."
      ],
      "metadata": {
        "id": "hcLQcCuBwZWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "densenet201_triv_aug_trans = transforms.Compose([\n",
        "    transforms.Resize(size=(256, 256)),\n",
        "    transforms.TrivialAugmentWide(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Trivial augment train data\n",
        "train_data_triv_aug = CustomImageDataset(X_train, y_train, data_transform=densenet201_triv_aug_trans)\n",
        "\n",
        "# Create different size batches of trivial aug train loaders\n",
        "train_dataloader_triv_aug_16 = DataLoader(dataset=train_data_triv_aug,\n",
        "                              batch_size=16,\n",
        "                              shuffle=True)\n",
        "\n",
        "train_dataloader_triv_aug_32 = DataLoader(dataset=train_data_triv_aug,\n",
        "                              batch_size=32,\n",
        "                              shuffle=True)\n",
        "\n",
        "train_dataloader_triv_aug_64 = DataLoader(dataset=train_data_triv_aug,\n",
        "                              batch_size=64,\n",
        "                              shuffle=True)"
      ],
      "metadata": {
        "id": "69oTf4KNwc5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = [5, 10]\n",
        "lrs = [.001, .0001, .01]\n",
        "batch_size = [16, 32, 64]\n",
        "\n",
        "hyperparameter_combos = list(itertools.product(batch_size, num_epochs, lrs))\n",
        "results = []"
      ],
      "metadata": {
        "id": "RTBBhiaHw7xW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------------------------------Hyperparameter Testing--------------------------------------------------\n",
        "# Create two datasets, one for each transform\n",
        "train_data_triv_aug = CustomImageDataset(X_train, y_train, data_transform=densenet201_triv_aug_trans)\n",
        "train_data_d201_transform = CustomImageDataset(X_train, y_train, data_transform=densenet201_train_datatransform)\n",
        "\n",
        "for transform_name, train_data in [(\"triv_aug\", train_data_triv_aug), (\"d201_transform\", train_data_d201_transform)]:\n",
        "    for batch_size, epoch, lr in hyperparameter_combos:\n",
        "        # Create DataLoader based on the current transform\n",
        "        train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
        "        save_dir = f\"runs/{transform_name}/lr_{lr}_batch_{batch_size}_epochs_{epoch}\"\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        writer = SummaryWriter(log_dir=save_dir)\n",
        "\n",
        "        params = {'learning_rate': lr, 'batch_size': batch_size, 'epochs': epoch, 'transform': transform_name}\n",
        "        writer.add_hparams(params, {})\n",
        "\n",
        "        print(f\"Starting training with hyperparameters: lr={lr}, batch_size={batch_size}, epochs={epoch}, transform={transform_name}\")\n",
        "\n",
        "        try:\n",
        "            train_results = train(model=model_d201,\n",
        "                                  train_dataloader=train_dataloader,\n",
        "                                  test_dataloader=val_dataloader,\n",
        "                                  optimizer=torch.optim.Adam(model_d201.parameters(), lr=lr),\n",
        "                                  loss_fn=criterion,\n",
        "                                  epochs=epoch,\n",
        "                                  device=device,\n",
        "                                  writer=writer,\n",
        "                                  params=params)\n",
        "\n",
        "            final_test_loss = train_results['test_loss'][-1]\n",
        "            final_test_ham_acc = train_results['test_ham_acc'][-1]\n",
        "            final_test_zero_one_acc = train_results['test_zero_one_acc'][-1]\n",
        "            writer.add_hparams(params, {'Final Test Loss': final_test_loss, 'Final Test Hamming Acc': final_test_ham_acc, 'Final Test Zero-One Acc': final_test_zero_one_acc})\n",
        "\n",
        "            results.append({'transform': transform_name, 'batch_size': batch_size, 'epochs': epoch, 'lr': lr, 'final_test_loss': final_test_loss, 'final_test_ham_acc': final_test_ham_acc, 'final_test_zero_one_acc': final_test_zero_one_acc})\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during training: {e}\")\n",
        "            results.append({'transform': transform_name, 'batch_size': batch_size, 'epochs': epoch, 'lr': lr, 'final_test_loss': float('nan'), 'final_test_ham_acc': float('nan'), 'final_test_zero_one_acc': float('nan')})\n",
        "\n",
        "        finally:\n",
        "            writer.close()\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)"
      ],
      "metadata": {
        "id": "8dG9ockExBNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save result images\n",
        "save_dir = os.getcwd()"
      ],
      "metadata": {
        "id": "_m_TBmZ4x-8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = pd.DataFrame(results)\n",
        "results_df['final_test_zero_one_acc'] = results_df['final_test_zero_one_acc'].apply(lambda x: x.item())\n",
        "results_df.to_csv('hyperparameter_testing_densenet201.csv', index=False)"
      ],
      "metadata": {
        "id": "pNsSvqf-yAY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_row = results_df.loc[results_df['final_test_ham_acc'].idxmax()]\n",
        "\n",
        "best_params = {\n",
        "    'transform': best_row['transform'],\n",
        "    'batch_size': best_row['batch_size'].item(),\n",
        "    'epochs': best_row['epochs'].item(),\n",
        "    'lr': best_row['lr'].item(),\n",
        "    'final_test_loss': best_row['final_test_loss'].item(),\n",
        "    'final_test_ham_acc': best_row['final_test_ham_acc'].item(),\n",
        "    'final_test_zero_one_acc': best_row['final_test_zero_one_acc'].item()\n",
        "}\n",
        "\n",
        "filepath = 'best_params_d201.json'\n",
        "with open(filepath, 'w') as f:\n",
        "    json.dump(best_params, f, indent=4)\n",
        "print(f\"Best parameters saved to {filepath}\")\n",
        "print(best_params)"
      ],
      "metadata": {
        "id": "tl36d_CqyNDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluations\n",
        "\n",
        "---\n",
        "After training, the model is evaluated on a held-out test set using several metrics. These include:\n",
        "*   **Accuracy:** Overall correctness of predictions.\n",
        "*   **Precision:** Proportion of correctly predicted positive labels among all predicted positive labels.\n",
        "*   **Recall:** Proportion of correctly predicted positive labels among all actual positive labels.\n",
        "*   **F1-Score:** Harmonic mean of precision and recall\n",
        "*   **Confusion Matrix:** Visual represnetation of model predictions versus ground truth.\n",
        "*   **ROC AUC:** Area under the ROC curve, measuring the model's ability to distinguish between classes.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fwMrBDmUE4Jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def how_did_i_do(model, test_dataloader, device):\n",
        "    \"\"\"Evaluates the model and computes metrics.\"\"\"\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_prob = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            inputs = batch['Image'].to(device)\n",
        "            labels = batch['Label'].to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            probabilities = torch.sigmoid(outputs)\n",
        "            y_prob.extend(probabilities.cpu().numpy())\n",
        "\n",
        "    y_true = np.array(y_true)\n",
        "    y_prob = np.array(y_prob)\n",
        "\n",
        "    y_pred = (y_prob >= 0.5).astype(int)\n",
        "\n",
        "    accuracy = compute_accuracy(y_true, y_pred)\n",
        "    precision = compute_precision(y_true, y_pred, average='micro')\n",
        "    recall = compute_recall(y_true, y_pred, average='micro')\n",
        "    f1 = compute_f1_score(y_true, y_pred, average='micro')\n",
        "    confusion_mat = compute_confusion_matrix(y_true.flatten(), y_pred.flatten())\n",
        "    roc_auc = compute_roc_auc(y_true.flatten(), y_prob.flatten())\n",
        "    fpr, tpr, roc_auc_value = compute_roc_curve_data(y_true.flatten(), y_prob.flatten())\n",
        "\n",
        "\n",
        "\n",
        "    # Print the metrics\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"F1-Score: {f1}\")\n",
        "    print(f\"Confusion Matrix:\\n{confusion_mat}\")\n",
        "    print(f\"ROC AUC: {roc_auc}\")\n",
        "    print(f\"FPR: {fpr}\")\n",
        "    print(f\"TPR: {tpr}\")\n",
        "    print(f\"ROC AUC Value: {roc_auc_value}\")\n",
        "\n",
        "    return accuracy, precision, recall, f1, confusion_mat, roc_auc, fpr, tpr, roc_auc_value\n"
      ],
      "metadata": {
        "id": "7pMi6rLCAPpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy, precision, recall, f1, confusion_mat, roc_auc, fpr, tpr, roc_auc_value = how_did_i_do(model_d201, test_dataloader, device)"
      ],
      "metadata": {
        "id": "xG3TMoyQAQbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizations\n",
        "\n",
        "---\n",
        "Finally, visualizations are generated to aid in the understanding of the model's performance. A confusion matrix is plotted to show the distribution of correct and incorre t predictions. Plots comparing training and validation loss and accuracy over epochs are generated as well as visualizations for the AUC.\n"
      ],
      "metadata": {
        "id": "gG9mzYniY-cM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion Matrix"
      ],
      "metadata": {
        "id": "5ES6SOoZgZS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(model_d201, test_dataloader, device, 'd201', save_dir)"
      ],
      "metadata": {
        "id": "osr2bKpYAp2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and Validation Comparisons per Metric"
      ],
      "metadata": {
        "id": "ZcvkfozeQevV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_validation_metrics(save_dir, train_results, 'd201')"
      ],
      "metadata": {
        "id": "RN2ZmCuHyVvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Area Under the Curve (AUC)"
      ],
      "metadata": {
        "id": "QU5JshLBQ50o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_roc_curves(save_dir, model_d201, test_dataloader, device, 'd201')"
      ],
      "metadata": {
        "id": "dfXN_WR7yZM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "1.   **Gallery of transformations:**<br>\n",
        "      PyTorch.org. \"Plot Transforms Illustrations.\" PyTorch, https://pytorch.org/vision/stable/auto_examples/transforms/plot_transforms_illustrations.html#sphx-glr-auto-examples-transforms-plot-transforms-illustrations-py. Accessed 01 November 2024.\n",
        "2.   **DataLoaders:**<br>\n",
        "      PyTorch.org. \"torch.utils.data.DataLoader — PyTorch 2.1.0+cu118 documentation.\" PyTorch, https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader. Accessed 03 November 2024.\n",
        "3.    **PyTorch Deep Learning (Train and Test Loops):**<br>\n",
        "      Bourke, Daniel. \"engine.py.\" pytorch-deep-learning, https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/engine.py. Accessed 03 November 2024.\n",
        "4.    **PyTorch Torchvision Blog Post:**<br>\n",
        "      PyTorch Blog. \"How to Train State-of-the-Art Models Using Torchvision’s Latest Primitives.\" PyTorch, https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/. Accessed 03 November 2024.\n",
        "5.    **DenseNet201 Tutorial:**<br>\n",
        "      Yesil Science. \"Transfer Learning DenseNet201.\" Yesil Science, https://yesilscience.com/transfer-learning-densenet201/. Accessed 20 November 2024.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GsxDNw4TZprN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rEIgLRoxdb84"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}