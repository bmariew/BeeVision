# -*- coding: utf-8 -*-
"""plot_utils.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LbuoQYToAevepWF1cKVd4ODaQ0xCOnKV

This file contains functions to generate various plots for visualizing the model's performance.
"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn.metrics import confusion_matrix, classification_report
import numpy as np
from sklearn.metrics import roc_curve, auc
import torch
import torch.nn as nn
import torch.optim as optim
import os

def sensitivity_analysis(df, hyperparameter, metric, fixed_params):
    """Performs sensitivity analysis for a given hyperparameter and metric."""
    print(f"Starting sensitivity analysis for {hyperparameter} with metric {metric}")
    best_params = df.loc[df[metric].idxmax()]
    print(f"Best parameters found: {best_params}")

    if hyperparameter == 'batch_size':
        param_range = range(max(1, int(best_params[hyperparameter]-10)), int(best_params[hyperparameter]+10))
    elif hyperparameter == 'epochs':
        param_range = range(max(1, int(best_params[hyperparameter]-2)), int(best_params[hyperparameter]+3))
    elif hyperparameter == 'lr':
        param_range = np.linspace(max(0.00001, best_params[hyperparameter]/10), best_params[hyperparameter]*10, 10)
    else:
        raise ValueError("Invalid hyperparameter specified.")

    results = []
    for val in param_range:
        temp_params = fixed_params.copy()
        temp_params[hyperparameter] = val

        if hyperparameter == 'lr':
            closest_row = df[
                (df['batch_size'] == temp_params['batch_size']) &
                (df['epochs'] == temp_params['epochs']) &
                np.isclose(df['lr'], temp_params['lr'], atol=1e-05) # <----------Adjust tolerance as needed
            ]
        else:
            closest_row = df[
                (df['batch_size'] == temp_params['batch_size']) &
                (df['epochs'] == temp_params['epochs']) &
                (df['lr'] == temp_params['lr'])
            ]

        if not closest_row.empty:
            results.append({'hyperparameter_value': val, 'metric_value': closest_row[metric].iloc[0]})
        else:
            results.append({'hyperparameter_value': val, 'metric_value': np.nan})

    return pd.DataFrame(results)

def run_sensitivity_analysis(df, hyperparameter, metric, fixed_params, model, save_dir):
    """Runs sensitivity analysis for a given hyperparameter and metric."""
    #fixed_params = df.loc[df['final_test_ham_acc'].idxmax()].to_dict()

    batch_size_analysis = sensitivity_analysis(df, 'batch_size', 'final_test_ham_acc', fixed_params)
    epochs_analysis = sensitivity_analysis(df, 'epochs', 'final_test_ham_acc', fixed_params)
    lr_analysis = sensitivity_analysis(df, 'lr', 'final_test_ham_acc', fixed_params)

    # Plot me
    plt.figure(figsize=(12, 4))

    plt.subplot(1, 3, 1)
    plt.plot(batch_size_analysis['hyperparameter_value'], batch_size_analysis['metric_value'], marker='o')
    plt.xlabel('Batch Size')
    plt.ylabel('Test Hamming Accuracy')
    plt.title('Sensitivity to Batch Size')
    plt.axvline(x=fixed_params['batch_size'], color='red', linestyle='--', label='Best Batch Size')
    plt.legend()

    plt.subplot(1, 3, 2)
    plt.plot(epochs_analysis['hyperparameter_value'], epochs_analysis['metric_value'], marker='o')
    plt.xlabel('Epochs')
    plt.ylabel('Test Hamming Accuracy')
    plt.title('Sensitivity to Epochs')
    plt.axvline(x=fixed_params['epochs'], color='red', linestyle='--', label='Best Epochs')
    plt.legend()

    plt.subplot(1, 3, 3)
    plt.plot(lr_analysis['hyperparameter_value'], lr_analysis['metric_value'], marker='o')
    plt.xlabel('Learning Rate')
    plt.ylabel('Test Hamming Accuracy')
    plt.title('Sensitivity to Learning Rate')
    plt.axvline(x=fixed_params['lr'], color='red', linestyle='--', label='Best Learning Rate')
    plt.legend()
    plt.savefig(os.path.join(save_dir, f"sensitivity_analysis_{model}.png"))
    plt.tight_layout()
    plt.show()

def plot_confusion_matrix(model, test_dataloader, device, model_name, save_dir):
    """Plots a confusion matrix."""

    model.eval()
    all_test_preds = []
    all_test_labels = []
    with torch.no_grad():
        for batch in test_dataloader:
            images = batch['Image'].to(device)
            labels = batch['Label'].to(device)
            preds = torch.sigmoid(model(images)) >= 0.5
            all_test_preds.append(preds.cpu())
            all_test_labels.append(labels.cpu())

    test_preds = torch.cat(all_test_preds).numpy()
    test_labels = torch.cat(all_test_labels).numpy()

    report = classification_report(test_labels, test_preds, target_names=[f"Label {i+1}" for i in range(15)])
    print(report)

    cm = confusion_matrix(test_labels.ravel(), test_preds.ravel())
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.title(f"Confusion Matrix - {model_name}")
    plt.savefig(os.path.join(save_dir, f"confusion_matrix_{model_name}.png"))
    plt.show()

def plot_roc_curves(save_dir, model, test_dataloader, device, model_name, title="Receiver Operating Characteristic (ROC) Curves"):
    """Plots ROC curves for multi-label classification."""
    model.eval()
    all_test_preds_probs = []
    all_test_labels = []
    with torch.no_grad():
        for batch in test_dataloader:
            images = batch['Image'].to(device)
            labels = batch['Label'].to(device)
            preds_probs = torch.sigmoid(model(images))
            all_test_preds_probs.append(preds_probs.cpu())
            all_test_labels.append(labels.cpu())

    test_preds_probs = torch.cat(all_test_preds_probs).numpy()
    test_labels = torch.cat(all_test_labels).numpy()

    plt.figure(figsize=(12, 8))
    for i in range(test_labels.shape[1]):
        fpr, tpr, thresholds = roc_curve(test_labels[:, i], test_preds_probs[:, i])
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f'Label {i+1} (AUC = {roc_auc:.2f})')

    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f"{title} - {model_name}")
    plt.legend(loc="lower right")
    plt.savefig(os.path.join(save_dir, f"ROC_{model_name}.png"))
    plt.show()

def plot_training_validation_metrics(save_dir, train_results, model_name, title="Training and Validation Metrics"):
    """Plots training and validation metrics including loss and accuracy."""
    train_losses = train_results['train_loss']
    train_ham_accs = train_results['train_ham_acc']
    train_zero_one_accs = train_results['train_zero_one_acc']
    test_losses = train_results['test_loss']
    test_ham_accs = train_results['test_ham_acc']
    test_zero_one_accs = train_results['test_zero_one_acc']
    epochs = range(1, len(train_losses) + 1)

    plt.figure(figsize=(12, 10))

    plt.subplot(3, 1, 1)
    plt.plot(epochs, train_losses, label='Training Loss')
    plt.plot(epochs, test_losses, label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title(f'Training and Validation Loss - {model_name}')
    plt.legend()

    plt.subplot(3, 1, 2)
    plt.plot(epochs, train_ham_accs, label='Training Hamming Accuracy')
    plt.plot(epochs, test_ham_accs, label='Validation Hamming Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.title(f'Training and Validation Hamming Accuracy - {model_name}')
    plt.legend()

    plt.subplot(3, 1, 3)
    plt.plot(epochs, train_zero_one_accs, label='Training Zero-One Accuracy')
    plt.plot(epochs, test_zero_one_accs, label='Validation Zero-One Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.title(f'Training and Validation Zero-One Accuracy - {model_name}')
    plt.legend()
    plt.savefig(os.path.join(save_dir, f"training_and_Validation_{model_name}.png"))
    plt.tight_layout()
    plt.show()