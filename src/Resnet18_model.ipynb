{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ2k0bv0Iu17"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "---\n",
        "This notebook details the complete workflow for building and evaluating a multi-label image classification model using the Resnet-18 architecture. The goal is to predict multiple labels simultaneously for each image within a dataset.  This notebook uses PyTorch for model building and training, scikit-learn for data splitting and some evaluation metrics.  The process is divided into several key steps, outlined below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmCeS0e_XaqN"
      },
      "source": [
        "# Import Libraries\n",
        "\n",
        "---\n",
        "This section imports all the necessary Python libraries required for the project. Key libraries include PyTorch (for deep learning, scikit-learn (for machine learning utilities), pandas (for data manipulation), and several others for image processing, visualization, and progress tracking.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOW-MCK8UB_4"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm.auto import tqdm\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from torchvision import models, transforms\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import itertools\n",
        "from sklearn.metrics import classification_report\n",
        "import random\n",
        "import json\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "from plot_utils import *\n",
        "from helper import *\n",
        "from metrics import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hx7kiPG7UFKf",
        "outputId": "b37ac238-4a34-45e8-af94-772a0d223176"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# # Set device to GPU if available, otherwise default to CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTaSj76tJAfZ"
      },
      "outputs": [],
      "source": [
        "#be fruitful and multiply\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "worB1ISLX9nC"
      },
      "source": [
        "# Load Files\n",
        "\n",
        "---\n",
        "This step loads the image data and corresponding labels. The image paths are read from a CSV file ('annotations.csv'), which contains filenames and their associated labels with associated feature presence. The images the dataset used for this project is a collection of images of beehive frames with varying features. The images themselves are loaded from the 'images' directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XDuxJmcUH2n"
      },
      "outputs": [],
      "source": [
        "data_dir = 'data'\n",
        "csv_path = os.path.join(data_dir, 'annotations.csv')\n",
        "img_path = os.path.join(data_dir, 'images')\n",
        "df = pd.read_csv(csv_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4-74mIkJdWO"
      },
      "source": [
        "# Model Definition and Training\n",
        "\n",
        "---\n",
        "\n",
        "A custom dataset class ('CustomImageDataset') is defined to efficiently handle the image and label data during training. The pre-trained Resnet-18 model is loaded, and its final classification layer is adjusted to match the number of output labels (15 in this case). The model is then trained using binary cross-entropy loss ('BCEWithLogitsLoss') and an Adam optimizer. The training process is monitored and logged, with metrics recorded at each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YE5UDYjgUIZO"
      },
      "outputs": [],
      "source": [
        "# *******************************************\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, images, labels, data_transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = data_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Apply transformers if provided\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Convert label from a list to a tensor\n",
        "        label = torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "        sample = {'Image': image, 'Label': label}\n",
        "\n",
        "        return sample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "29geVmVWou8L"
      },
      "outputs": [],
      "source": [
        "# Read in label file (annotations.csv)\n",
        "df = pd.read_csv(csv_path)\n",
        "#df = df[:20]\n",
        "\n",
        "\n",
        "# Convert all labels to floats, NaN if blank\n",
        "for col in df.columns[1:16]:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "# Drop any images with NaNs in label columns\n",
        "df.dropna(subset=df.columns[:16], inplace=True)\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Extract ground truth labels for each image in label file (annotations.csv)\n",
        "labels = []\n",
        "for index, row in df.iterrows():\n",
        "    labels.append(list(row.iloc[1:16]))\n",
        "\n",
        "# Create path for and load in each image in directory\n",
        "image_names = df['filename'].tolist()\n",
        "images = []\n",
        "\n",
        "for img_name in tqdm(image_names):\n",
        "    image_path = os.path.join(img_path, img_name)\n",
        "    image = Image.open(image_path)\n",
        "    if image.mode != 'RGB':\n",
        "        image = image.convert('RGB')\n",
        "    images.append(image)\n",
        "\n",
        "# Split data into train (80%), validation (10%), and test (10%) sets\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(images, labels, test_size=1/10, random_state=seed)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=1/9, random_state=seed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mME4d_gWA9Ls"
      },
      "source": [
        "### Feature Distribution on the split sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "bKqKLCFeA7cT"
      },
      "outputs": [],
      "source": [
        "feature_distribution_train = pd.DataFrame(np.array(y_train).sum(axis=0), columns=['count'], index=df.columns[1:16])\n",
        "# at_the_bar(feature_distribution_train['count'], 'Feature Distribution - Training Set', 'feature_distribution_train.png', 'res18', 'train', save_dir)a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "LXYlXwouBHmm"
      },
      "outputs": [],
      "source": [
        "feature_distribution_val = pd.DataFrame(np.array(y_val).sum(axis=0), columns=['count'], index=df.columns[1:16])\n",
        "at_the_bar(feature_distribution_val['count'], 'Feature Distribution - Validation Set', 'feature_distribution_val.png', 'res18', 'val', save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "hrrJJ5EZBJ7J"
      },
      "outputs": [],
      "source": [
        "feature_distribution_test = pd.DataFrame(np.array(y_test).sum(axis=0), columns=['count'], index=df.columns[1:16])\n",
        "at_the_bar(feature_distribution_test['count'], 'Feature Distribution - Test Set', 'feature_distribution_test.png', 'res18', 'test', save_dir)a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOpS4z2GUIbh"
      },
      "outputs": [],
      "source": [
        "res18_train_datatransform = transforms.Compose([\n",
        "    transforms.Resize(size=(224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                          std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "res18_test_datatransform = transforms.Compose([\n",
        "    transforms.Resize(size=(224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "res18_triv_aug_trans = transforms.Compose([\n",
        "    transforms.Resize(size=(224, 224)),\n",
        "    transforms.TrivialAugmentWide(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "JNo2wDo2UIdt"
      },
      "outputs": [],
      "source": [
        "# *********************************\n",
        "# Create datasets for each split\n",
        "val_data = CustomImageDataset(X_val, y_val, data_transform=res18_test_datatransform)\n",
        "test_data = CustomImageDataset(X_test, y_test, data_transform=res18_test_datatransform)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "val_dataloader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Inherit the pre-trained weights from the ResNet18 model\n",
        "weights = models.ResNet18_Weights.DEFAULT\n",
        "model_res18 = models.resnet18(weights=weights).to(device)\n",
        "\n",
        "for param in model_res18.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "# Set output_shape variable to the number of labels in dataset\n",
        "input_shape = model_res18.fc.in_features\n",
        "output_shape = 15\n",
        "\n",
        "\n",
        "# Recreate the `fc` layer of the pre-trained model to custom output shape\n",
        "model_res18.fc = torch.nn.Sequential(\n",
        "    torch.nn.Linear(input_shape,\n",
        "                    output_shape),\n",
        "                    #nn.Sigmoid()\n",
        ").to(device)\n",
        "\n",
        "#Optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model_res18.parameters(), lr=0.001)\n",
        "\n",
        "writer = SummaryWriter(log_dir=\"runs/my_resnet18_run\")\n",
        "params = {'learning_rate': 0.001, 'batch_size': 32, 'epochs':10}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hCYgfKBXtKK"
      },
      "source": [
        "# Hyperparameter Tuning\n",
        "\n",
        "---\n",
        "\n",
        "To optimize performance, a hyperparameter search is conducted. Different combinations of batch size, number of epochs, and learning rate are tested, using both the original data transformations and a new transformation using 'transforms.TriviaAugmentWide'. The results of this search are saved to 'hyperparaeter_testing_Res18.csv' for analysis. The best performing hyperparameter set is then saved to 'best_params_res18'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNlOhD6ZOSwG"
      },
      "outputs": [],
      "source": [
        "num_epochs = [5, 10]\n",
        "lrs = [.001, .0001, .01]\n",
        "batch_size = [16, 32, 64]\n",
        "\n",
        "hyperparameter_combos = list(itertools.product(batch_size, num_epochs, lrs))\n",
        "results = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "O01LSbNBoTvK"
      },
      "outputs": [],
      "source": [
        "# Create two datasets, one for each transform\n",
        "train_data_triv_aug = CustomImageDataset(X_train, y_train, data_transform=res18_triv_aug_trans)\n",
        "train_data_res18_transform = CustomImageDataset(X_train, y_train, data_transform=res18_train_datatransform)\n",
        "\n",
        "for transform_name, train_data in [(\"triv_aug\", train_data_triv_aug), (\"res18_transform\", train_data_res18_transform)]:\n",
        "    for batch_size, epoch, lr in hyperparameter_combos:\n",
        "        # Create DataLoader based on the current transform\n",
        "        train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
        "        save_dir = f\"runs/{transform_name}/lr_{lr}_batch_{batch_size}_epochs_{epoch}\"\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        writer = SummaryWriter(log_dir=save_dir)\n",
        "\n",
        "        params = {'learning_rate': lr, 'batch_size': batch_size, 'epochs': epoch, 'transform': transform_name}\n",
        "        writer.add_hparams(params, {})\n",
        "\n",
        "        print(f\"Starting training with hyperparameters: lr={lr}, batch_size={batch_size}, epochs={epoch}, transform={transform_name}\")\n",
        "\n",
        "        try:\n",
        "            train_results = train(model=model_res18,\n",
        "                                  train_dataloader=train_dataloader,\n",
        "                                  test_dataloader=val_dataloader,\n",
        "                                  optimizer=torch.optim.Adam(model_res18.parameters(), lr=lr),\n",
        "                                  loss_fn=criterion,\n",
        "                                  epochs=epoch,\n",
        "                                  device=device,\n",
        "                                  writer=writer,\n",
        "                                  params=params)\n",
        "\n",
        "            final_test_loss = train_results['test_loss'][-1]\n",
        "            final_test_ham_acc = train_results['test_ham_acc'][-1]\n",
        "            final_test_zero_one_acc = train_results['test_zero_one_acc'][-1]\n",
        "            writer.add_hparams(params, {'Final Test Loss': final_test_loss, 'Final Test Hamming Acc': final_test_ham_acc, 'Final Test Zero-One Acc': final_test_zero_one_acc})\n",
        "\n",
        "            results.append({'transform': transform_name, 'batch_size': batch_size, 'epochs': epoch, 'lr': lr, 'final_test_loss': final_test_loss, 'final_test_ham_acc': final_test_ham_acc, 'final_test_zero_one_acc': final_test_zero_one_acc})\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during training: {e}\")\n",
        "            results.append({'transform': transform_name, 'batch_size': batch_size, 'epochs': epoch, 'lr': lr, 'final_test_loss': float('nan'), 'final_test_ham_acc': float('nan'), 'final_test_zero_one_acc': float('nan')})\n",
        "\n",
        "        finally:\n",
        "            writer.close()\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTDjVagNi8sc"
      },
      "outputs": [],
      "source": [
        "results_df = pd.DataFrame(results)\n",
        "results_df['final_test_zero_one_acc'] = results_df['final_test_zero_one_acc'].apply(lambda x: x.item())\n",
        "file_path = os.path.join(save_dir, \"hyperparameter_testing_Res18.csv\")\n",
        "results_df.to_csv(file_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oKh_iXnKz_V",
        "outputId": "76c8d854-fc0a-4a85-86dc-081fc880420a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters saved to best_params_res18.json\n",
            "{'transform': 'triv_aug', 'batch_size': 16, 'epochs': 5, 'lr': 0.0001, 'final_test_loss': 0.2857619822025299, 'final_test_ham_acc': 0.87458336353302, 'final_test_zero_one_acc': 0.109375}\n"
          ]
        }
      ],
      "source": [
        "best_row = results_df.loc[results_df['final_test_ham_acc'].idxmax()]\n",
        "\n",
        "best_params = {\n",
        "    'transform': best_row['transform'],\n",
        "    'batch_size': best_row['batch_size'].item(),\n",
        "    'epochs': best_row['epochs'].item(),\n",
        "    'lr': best_row['lr'].item(),\n",
        "    'final_test_loss': best_row['final_test_loss'].item(),\n",
        "    'final_test_ham_acc': best_row['final_test_ham_acc'].item(),\n",
        "    'final_test_zero_one_acc': best_row['final_test_zero_one_acc'].item()\n",
        "}\n",
        "\n",
        "filepath = 'best_params_res18.json'\n",
        "with open(filepath, 'w') as f:\n",
        "    json.dump(best_params, f, indent=4)\n",
        "print(f\"Best parameters saved to {filepath}\")\n",
        "print(best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVJG9jqt8hc5"
      },
      "outputs": [],
      "source": [
        "# load best hyperparameters\n",
        "filepath = 'best_params_res18.json'\n",
        "with open(filepath, 'r') as f:\n",
        "    best_params = json.load(f)\n",
        "\n",
        "best_transform = best_params['transform']\n",
        "if best_transform == 'triv_aug':\n",
        "    train_data = CustomImageDataset(X_train, y_train, data_transform=res18_triv_aug_trans)\n",
        "elif best_transform == 'res18_transform':\n",
        "    train_data = CustomImageDataset(X_train, y_train, data_transform=res18_train_datatransform)\n",
        "else:\n",
        "    raise ValueError(f\"Unknown transform: {best_transform}\")\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size=best_params['batch_size'], shuffle=True)\n",
        "val_dataloader = DataLoader(val_data, batch_size=best_params['batch_size'], shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Ldq8oj6_8yyI"
      },
      "outputs": [],
      "source": [
        "#train with best hyperparameters\n",
        "optimizer = torch.optim.Adam(model_res18.parameters(), lr=best_params['lr'])\n",
        "train_results = train(model=model_res18,\n",
        "                      train_dataloader=train_dataloader,\n",
        "                      test_dataloader=val_dataloader,\n",
        "                      optimizer=optimizer,\n",
        "                      loss_fn=criterion,\n",
        "                      epochs=best_params['epochs'],\n",
        "                      device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZjwbAFw84Z7"
      },
      "source": [
        "### Final Testing on the held-out test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Nyq3UPpV89gF"
      },
      "outputs": [],
      "source": [
        "test_loss, test_ham_acc, test_zero_one_acc = test_step(model=model_res18,\n",
        "                                                        dataloader=test_dataloader,\n",
        "                                                        loss_fn=criterion,\n",
        "                                                        device=device)\n",
        "\n",
        "test_loss = test_loss.item() if isinstance(test_loss, torch.Tensor) else test_loss\n",
        "test_ham_acc = test_ham_acc.item() if isinstance(test_ham_acc, torch.Tensor) else test_ham_acc\n",
        "test_zero_one_acc = test_zero_one_acc.item() if isinstance(test_zero_one_acc, torch.Tensor) else test_zero_one_acc\n",
        "\n",
        "test_results = {\n",
        "    'Test Loss': test_loss,\n",
        "    'Test Hamming Accuracy': test_ham_acc,\n",
        "    'Test Zero-One Accuracy': test_zero_one_acc\n",
        "}\n",
        "\n",
        "json_filepath = os.path.join(save_dir, 'test_results_res18.json')\n",
        "with open(json_filepath, 'w') as jsonfile:\n",
        "    json.dump(test_results, jsonfile, indent=4)\n",
        "\n",
        "print(f\"Test results saved to {json_filepath}\")\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Hamming Accuracy: {test_ham_acc:.4f}, Test Zero-One Accuracy: {test_zero_one_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ie441ORXixg9"
      },
      "source": [
        "# Evaluations\n",
        "\n",
        "---\n",
        "After training, the model is evaluated on a held-out test set using several metrics. These include:\n",
        "*   **Accuracy:** Overall correctness of predictions.\n",
        "*   **Precision:** Proportion of correctly predicted positive labels among all predicted positive labels.\n",
        "*   **Recall:** Proportion of correctly predicted positive labels among all actual positive labels.\n",
        "*   **F1-Score:** Harmonic mean of precision and recall\n",
        "*   **Confusion Matrix:** Visual represnetation of model predictions versus ground truth.\n",
        "*   **ROC AUC:** Area under the ROC curve, measuring the model's ability to distinguish between classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "iqy2hwo1hRjc"
      },
      "outputs": [],
      "source": [
        "def how_did_i_do(model, test_dataloader, device):\n",
        "    \"\"\"Evaluates the model and computes metrics.\"\"\"\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_prob = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            inputs = batch['Image'].to(device)\n",
        "            labels = batch['Label'].to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            probabilities = torch.sigmoid(outputs)\n",
        "            y_prob.extend(probabilities.cpu().numpy())\n",
        "\n",
        "    y_true = np.array(y_true)\n",
        "    y_prob = np.array(y_prob)\n",
        "\n",
        "    y_pred = (y_prob >= 0.5).astype(int)\n",
        "\n",
        "    accuracy = compute_accuracy(y_true, y_pred)\n",
        "    precision = compute_precision(y_true, y_pred, average='micro')\n",
        "    recall = compute_recall(y_true, y_pred, average='micro')\n",
        "    f1 = compute_f1_score(y_true, y_pred, average='micro')\n",
        "    confusion_mat = compute_confusion_matrix(y_true.flatten(), y_pred.flatten())\n",
        "    roc_auc = compute_roc_auc(y_true.flatten(), y_prob.flatten())\n",
        "    fpr, tpr, roc_auc_value = compute_roc_curve_data(y_true.flatten(), y_prob.flatten())\n",
        "\n",
        "\n",
        "    # Print the metrics\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"F1-Score: {f1}\")\n",
        "    print(f\"Confusion Matrix:\\n{confusion_mat}\")\n",
        "    print(f\"ROC AUC: {roc_auc}\")\n",
        "    print(f\"FPR: {fpr}\")\n",
        "    print(f\"TPR: {tpr}\")\n",
        "    print(f\"ROC AUC Value: {roc_auc_value}\")\n",
        "\n",
        "    return accuracy, precision, recall, f1, confusion_mat, roc_auc, fpr, tpr, roc_auc_value, y_true, y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "6lzWCEdyhTUS"
      },
      "outputs": [],
      "source": [
        "accuracy, precision, recall, f1, confusion_mat, roc_auc, fpr, tpr, roc_auc_value, y_true, y_pred = how_did_i_do(model_res18, test_dataloader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVLeBgbcpRgb"
      },
      "source": [
        "# Visualizations\n",
        "\n",
        "---\n",
        "Finally, visualizations are generated to aid in the understanding of the model's performance. A confusion matrix is plotted to show the distribution of correct and incorrect predictions. Plots comparing training and validation loss and accuracy over epochs are generated as well as visualizations for the AUC.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTkw3qSOYoav"
      },
      "source": [
        "### Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "1q70gyxe_U-H"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix(y_true, y_pred, df, 'res18', save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "tE8jQjjDoE9m"
      },
      "outputs": [],
      "source": [
        "plot_single_confusion_matrix(model_res18, test_dataloader, device, 'res18', save_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlKfqFXpLnsE"
      },
      "source": [
        "### Training and Validation Comaprisons per Metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "kcvi8AUb6YBq"
      },
      "outputs": [],
      "source": [
        "plot_training_validation_metrics(save_dir, train_results, 'res18')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkGd3xdcLqxj"
      },
      "source": [
        "### Area Under the Curve (AUC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Z0Rf4Te7B4dK"
      },
      "outputs": [],
      "source": [
        "plot_roc_curves(save_dir, model_res18, test_dataloader, device, 'res18')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VICS04tLwSd"
      },
      "source": [
        "# Tensor Board"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taEnqpIqCQYJ"
      },
      "outputs": [],
      "source": [
        "# Viewing TensorBoard in Jupyter and Google Colab Notebooks (uncomment to view full TensorBoard instance)\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_ZYIMF-L1Za"
      },
      "source": [
        "# References\n",
        "---\n",
        "1.   **Gallery of transformations:**<br>\n",
        "      PyTorch.org. \"Plot Transforms Illustrations.\" PyTorch, https://pytorch.org/vision/stable/auto_examples/transforms/plot_transforms_illustrations.html#sphx-glr-auto-examples-transforms-plot-transforms-illustrations-py. Accessed 01 November 2024.\n",
        "2.   **DataLoaders:**<br>\n",
        "      PyTorch.org. \"torch.utils.data.DataLoader — PyTorch 2.1.0+cu118 documentation.\" PyTorch, https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader. Accessed 03 November 2024.\n",
        "3.    **PyTorch Deep Learning (Train and Test Loops):**<br>\n",
        "      Bourke, Daniel. \"engine.py.\" pytorch-deep-learning, https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/engine.py. Accessed 03 November 2024.\n",
        "4.    **PyTorch Torchvision Blog Post:**<br>\n",
        "      PyTorch Blog. \"How to Train State-of-the-Art Models Using Torchvision’s Latest Primitives.\" PyTorch, https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/. Accessed 03 November 2024.\n",
        "5.    **Resnet-18 Video Tutorial:**<br>\n",
        "      Indomitable Tech. (2022, June 27). \"Implement a PreTrained (ResNet18) CNN Model using PyTorch from scratch on a Kaggle Image Dataset\". [Online Video]. YouTube. https://www.youtube.com/watch?v=5rD8f1oiuWM. Accessed 18 November 2024.\n",
        "6.    **Resnet-18 Tutorial:**<br>\n",
        "      Srivastava, Gaurav. \"Implementing ResNet18 for Image Classification\". Kaggle. https://www.kaggle.com/code/ggsri123/implementing-resnet18-for-image-classification. Accessed 18 November 2024.\n",
        "7.    **Tensor Board Tutorial:**<br>\n",
        "      TensorFlow. (n.d.). Get started with TensorBoard. https://www.tensorflow.org/tensorboard/get_started. 22 November 2024."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UuD3A7DtwL3z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}